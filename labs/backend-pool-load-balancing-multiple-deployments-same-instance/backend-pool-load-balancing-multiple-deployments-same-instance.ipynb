{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIM ‚ù§Ô∏è OpenAI\n",
    "\n",
    "## Backend pool Load Balancing Multiple Deployments on same Azure OpenAI Instances lab\n",
    "\n",
    "Playground to try the built-in load balancing [backend pool functionality of APIM](https://learn.microsoft.com/en-us/azure/api-management/backends?tabs=bicep) where two Azure OpenAI endpoints both host the same deployments.\n",
    "\n",
    "Notes:\n",
    "- The backend pool uses round-robin by default\n",
    "- But priority and weight based routing are also supported: Adjust the `priority` (the lower the number, the higher the priority) and `weight` parameters in the `openai_resources` variable\n",
    "- The `retry` API Management policy initiates a retry to an available backend if an HTTP 429 status code is encountered\n",
    "\n",
    "### TOC\n",
    "- [0Ô∏è‚É£ Initialize notebook variables](#0)\n",
    "- [1Ô∏è‚É£ Create the Azure Resource Group](#1)\n",
    "- [2Ô∏è‚É£ Create deployment using ü¶æ Bicep](#2)\n",
    "- [3Ô∏è‚É£ Get the deployment outputs](#3)\n",
    "- [üß™ Test the API using a direct HTTP call](#requests)\n",
    "- [üîç Analyze Load Balancing results](#plot)\n",
    "- [üß™ Test the API using the Azure OpenAI Python SDK](#sdk)\n",
    "- [üóëÔ∏è Clean up resources](#clean)\n",
    "\n",
    "### Prerequisites\n",
    "- [Python 3.8 or later version](https://www.python.org/) installed\n",
    "- [Pandas Library](https://pandas.pydata.org/) and matplotlib installed\n",
    "- [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n",
    "- [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) installed\n",
    "- [An Azure Subscription](https://azure.microsoft.com/en-us/free/) with Contributor permissions\n",
    "- [Access granted to Azure OpenAI](https://aka.ms/oai/access) or just enable the mock service\n",
    "- [Sign in to Azure with Azure CLI](https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli-interactively)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 0Ô∏è‚É£ Initialize notebook variables\n",
    "\n",
    "- Resources will be suffixed by a unique string based on your subscription id\n",
    "- The ```mock_webapps``` variable sets the list of deployed Web Apps for the mocking functionality. Clean the ```openai_resources``` list to simulate the OpenAI behaviour with the mocking service.\n",
    "- Adjust the location parameters according your preferences and on the [product availability by Azure region.](https://azure.microsoft.com/en-us/explore/global-infrastructure/products-by-region/?cdn=disable&products=cognitive-services,api-management) \n",
    "- Adjust the OpenAI model and version according the [availability by region.](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import requests\n",
    "\n",
    "deployment_name = os.path.basename(os.path.dirname(globals()['__vsc_ipynb_file__']))\n",
    "resource_group_name = f\"lab-{deployment_name}\" # change the name to match your naming style\n",
    "resource_group_location = \"westeurope\"\n",
    "apim_resource_name = \"apim\"\n",
    "apim_resource_location = \"westeurope\"\n",
    "apim_resource_sku = \"Basicv2\"\n",
    "openai_resources_sku = \"S0\"\n",
    "\n",
    "model1 = {\n",
    "    \"openai_model_name\": \"gpt-35-turbo\",\n",
    "    \"openai_model_version\": \"0613\",\n",
    "    \"openai_deployment_name\": \"gpt-35-turbo-0613\"\n",
    "}\n",
    "model2 = {\n",
    "    \"openai_model_name\": \"gpt-35-turbo\",\n",
    "    \"openai_model_version\": \"1106\",\n",
    "    \"openai_deployment_name\": \"gpt-35-turbo-1106\"\n",
    "}\n",
    "\n",
    "openai_resources = [\n",
    "    {\"name\": \"openai1\", \"location\": \"uksouth\", \"priority\": 1, \"weight\": 50, \"deployments\": [\"gpt-35-turbo-0613\", \"gpt-35-turbo-1106\"], \"models\": [model1, model2]},\n",
    "    {\"name\": \"openai2\", \"location\": \"swedencentral\", \"priority\": 1, \"weight\": 50, \"deployments\": [\"gpt-35-turbo-0613\", \"gpt-35-turbo-1106\"], \"models\": [model1, model2]}\n",
    "]\n",
    "\n",
    "#print(json.dumps(openai_resources, indent=4))\n",
    "\n",
    "openai_api_version = \"2024-02-01\"\n",
    "openai_specification_url='https://raw.githubusercontent.com/Azure/azure-rest-api-specs/main/specification/cognitiveservices/data-plane/AzureOpenAI/inference/stable/' + openai_api_version + '/inference.json'\n",
    "openai_backend_pool = \"openai-backend-pool\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "### 1Ô∏è‚É£ Create the Azure Resource Group\n",
    "All resources deployed in this lab will be created in the specified resource group. Skip this step if you want to use an existing resource group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_group_stdout = ! az group create --name {resource_group_name} --location {resource_group_location}\n",
    "if resource_group_stdout.n.startswith(\"ERROR\"):\n",
    "    print(resource_group_stdout)\n",
    "else:\n",
    "    print(\"‚úÖ Azure Resource Group \", resource_group_name, \" created ‚åö \", datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### 2Ô∏è‚É£ Create deployment using ü¶æ Bicep\n",
    "\n",
    "This lab uses [Bicep](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/overview?tabs=bicep) to declarative define all the resources that will be deployed. Change the parameters or the [main.bicep](main.bicep) directly to try different configurations. \n",
    "\n",
    "`openAIModelCapacity` is set intentionally low to `2` (2k tokens per minute) to showcase the retry logic in the load balancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicep_parameters = {\n",
    "  \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\",\n",
    "  \"contentVersion\": \"1.0.0.0\",\n",
    "  \"parameters\": {\n",
    "    \"openAIBackendPoolName\": { \"value\": openai_backend_pool },\n",
    "    \"openAIConfig\": { \"value\": openai_resources },\n",
    "    \"openAISku\": { \"value\": openai_resources_sku },\n",
    "    \"openAIModelCapacity\": { \"value\": 2 },\n",
    "    \"openAIAPISpecURL\": { \"value\": openai_specification_url },\n",
    "    \"apimResourceName\": { \"value\": apim_resource_name},\n",
    "    \"apimResourceLocation\": { \"value\": apim_resource_location},\n",
    "    \"apimSku\": { \"value\": apim_resource_sku}\n",
    "  }\n",
    "}\n",
    "with open('params.json', 'w') as bicep_parameters_file:\n",
    "    bicep_parameters_file.write(json.dumps(bicep_parameters))\n",
    "\n",
    "! az deployment group create --name {deployment_name} --resource-group {resource_group_name} --template-file \"main.bicep\" --parameters \"params.json\"\n",
    "\n",
    "# Retrieve the output\n",
    "# ! az deployment group show --name myDeployment --resource-group {resource_group_name} --query properties.outputs.uniqueDeploymentNames.value\n",
    "! az deployment group show --name myDeployment --resource-group {resource_group_name} --query \"properties.outputs.{flattenedOpenAIConfigFoo: flattenedOpenAIConfigFoo.value, distinctDeploymentNames: distinctDeploymentNames.value, uniqueDeploymentNames: uniqueDeploymentNames.value, backendsByDeploymentName: backendsByDeploymentName.value, backendsByLocation: backendsByLocation.value, backendsByPriority: backendsByPriority.value, backendsByWeight: backendsByWeight.value, backendsByLocationPriority: backendsByLocationPriority.value, backendsByLocationWeight: backendsByLocationWeight.value, backendsByLocationPriorityWeight: backendsByLocationPriorityWeight}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### 3Ô∏è‚É£ Get the deployment outputs\n",
    "\n",
    "We are now at the stage where we only need to retrieve the gateway URL and the subscription before we are ready for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_stdout = ! az deployment group show --name {deployment_name} -g {resource_group_name} --query properties.outputs.apimSubscriptionKey.value -o tsv\n",
    "apim_subscription_key = deployment_stdout.n\n",
    "deployment_stdout = ! az deployment group show --name {deployment_name} -g {resource_group_name} --query properties.outputs.apimResourceGatewayURL.value -o tsv\n",
    "apim_resource_gateway_url = deployment_stdout.n\n",
    "print(\"üëâüèª API Gateway URL: \", apim_resource_gateway_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### üß™ Test the API using a direct HTTP call\n",
    "Requests is an elegant and simple HTTP library for Python that will be used here to make raw API requests and inspect the responses. \n",
    "\n",
    "You will not see HTTP 429s returned as API Management's `retry` policy will select an available backend. If no backends are viable, an HTTP 503 will be returned.\n",
    "\n",
    "Tip: Use the [tracing tool](../../tools/tracing.ipynb) to track the behavior of the backend pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "runs = 20\n",
    "sleep_time_ms = 0\n",
    "url = apim_resource_gateway_url + \"/openai/deployments/\" + model1['openai_deployment_name'] + \"/chat/completions?api-version=\" + openai_api_version\n",
    "api_runs = []\n",
    "\n",
    "for i in range(runs):\n",
    "    print(\"‚ñ∂Ô∏è Run:\", i+1, \"/\", runs)\n",
    "\n",
    "    print(url)\n",
    "\n",
    "    if len(openai_resources) > 0:\n",
    "        messages={\"messages\":[\n",
    "            {\"role\": \"system\", \"content\": \"You are a sarcastic unhelpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "        ]}\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, headers = {'api-key':apim_subscription_key}, json = messages)\n",
    "    response_time = time.time() - start_time\n",
    "\n",
    "    print(f\"‚åö {response_time:.2f} seconds\")\n",
    "    # Check the response status code and apply formatting\n",
    "    if 200 <= response.status_code < 300:\n",
    "        status_code_str = '\\x1b[1;32m' + str(response.status_code) + \" - \" + response.reason + '\\x1b[0m'  # Bold and green\n",
    "    elif response.status_code >= 400:\n",
    "        status_code_str = '\\x1b[1;31m' + str(response.status_code) + \" - \" + response.reason + '\\x1b[0m'  # Bold and red\n",
    "    else:\n",
    "        status_code_str = str(response.status_code)  # No formatting\n",
    "\n",
    "    # Print the response status with the appropriate formatting\n",
    "    print(\"Response status:\", status_code_str)\n",
    "\n",
    "    print(\"Response headers:\", response.headers)\n",
    "\n",
    "    if \"x-ms-region\" in response.headers:\n",
    "        print(\"x-ms-region:\", '\\x1b[1;31m'+response.headers.get(\"x-ms-region\")+'\\x1b[0m') # this header is useful to determine the region of the backend that served the request\n",
    "        api_runs.append((response_time, response.headers.get(\"x-ms-region\")))\n",
    "\n",
    "    if (response.status_code == 200):\n",
    "        data = json.loads(response.text)\n",
    "        print(\"Token usage:\", data.get(\"usage\"), \"\\n\")\n",
    "        print(\"üí¨ \", data.get(\"choices\")[0].get(\"message\").get(\"content\"), \"\\n\")\n",
    "    else:\n",
    "        print(response.text)\n",
    "\n",
    "    time.sleep(sleep_time_ms/1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot'></a>\n",
    "### üîç Analyze Load Balancing results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Reenable this later.\n",
    "\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['figure.figsize'] = [15, 7]\n",
    "# df = pd.DataFrame(api_runs, columns=['Response Time', 'Region'])\n",
    "# df['Run'] = range(1, len(df) + 1)\n",
    "\n",
    "# # Define a color map for each region\n",
    "# color_map = {'UK South': 'lightpink', 'France Central': 'lightblue', 'Sweden Central': 'lightyellow', 'Region3': 'red', 'Region4': 'orange'}  # Add more regions and colors as needed\n",
    "\n",
    "# # Plot the dataframe with colored bars\n",
    "# ax = df.plot(kind='bar', x='Run', y='Response Time', color=[color_map.get(region, 'gray') for region in df['Region']], legend=False)\n",
    "\n",
    "# # Add legend\n",
    "# legend_labels = [plt.Rectangle((0, 0), 1, 1, color=color_map.get(region, 'gray')) for region in df['Region'].unique()]\n",
    "# ax.legend(legend_labels, df['Region'].unique())\n",
    "\n",
    "# plt.title('Load Balancing results')\n",
    "# plt.xlabel('Runs')\n",
    "# plt.ylabel('Response Time')\n",
    "# plt.xticks(df['Run'], rotation=0)\n",
    "\n",
    "# average = df['Response Time'].mean()\n",
    "# plt.axhline(y=average, color='r', linestyle='--', label=f'Average: {average:.2f}')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sdk'></a>\n",
    "### üß™ Test the API using the Azure OpenAI Python SDK\n",
    "\n",
    "Repeat the same test using the Python SDK to ensure compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "runs = 9\n",
    "sleep_time_ms = 0\n",
    "\n",
    "for i in range(runs):\n",
    "    print(\"‚ñ∂Ô∏è Run: \", i+1)\n",
    "\n",
    "    if len(openai_resources) > 0:\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a sarcastic unhelpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "        ]\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=apim_resource_gateway_url,\n",
    "        api_key=apim_subscription_key,\n",
    "        api_version=openai_api_version\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    response = client.chat.completions.create(model=model1[\"openai_deployment_name\"], messages=messages)\n",
    "    response_time = time.time() - start_time\n",
    "    print(f\"‚åö {response_time:.2f} seconds\")\n",
    "    print(\"üí¨ \", response.choices[0].message.content)\n",
    "    time.sleep(sleep_time_ms/1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "### üóëÔ∏è Clean up resources\n",
    "\n",
    "When you're finished with the lab, you should remove all your deployed resources from Azure to avoid extra charges and keep your Azure subscription uncluttered.\n",
    "Use the [clean-up-resources notebook](clean-up-resources.ipynb) for that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
